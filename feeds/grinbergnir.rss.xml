<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Three.Fourteen</title><link>http://www.nirg.net/</link><description></description><atom:link href="http://www.nirg.net/feeds/grinbergnir.rss.xml" rel="self"></atom:link><lastBuildDate>Fri, 24 Feb 2017 14:39:00 -0500</lastBuildDate><item><title>When Do People Expect More Likes and Comments on Facebook and From Whom?</title><link>http://www.nirg.net/feedback-exp.html</link><description>&lt;p&gt;Despite the fact that social media has been around for over a decade now, we still know reletively little about people's expectations for getting attention and responses from their online friends. In &lt;a href="/papers/feedback_exp.pdf"&gt;this work&lt;/a&gt; we examine the often overlooked end of the attention ``transaction'': the content producer's expectation to be heard. This study offers both a conceptual framework for thinking about feedback expectations and a computational model that can be used in practice. Our conceptual framework describes the factors affecting expectations for getting feedback from friends and family, and the implications for the individual of fulfilling those expectations. By combining surveys and observational data analysis we provide evidence for the relation between observed behaviors and expectations. We also show that the fulfillment of expectations contributes to people's sense of connectedness to their friends, an important outcome for individuals' well being (see Burke, Marlow &amp;amp; Lento 2010; Kivran-Swaine, Ting, Brubaker, Teodoro &amp;amp; Naaman 2014). In addition, our simple and easily portable predictive models demonstrate how the expectations of content producers can be incorporated in recommendation systems in practice, a necessary step towards building less ego-centric recommendation systems for the consumption of social content.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="/pages/publications.html"&gt;publications&lt;/a&gt; page for the full paper. &lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">grinbergnir</dc:creator><pubDate>Fri, 24 Feb 2017 14:39:00 -0500</pubDate><guid>tag:www.nirg.net,2017-02-24:feedback-exp.html</guid></item><item><title>Changes in Engagement Before and After Posting to Facebook</title><link>http://www.nirg.net/contribution-effect.html</link><description>&lt;p&gt;Our recent work got accepted to CHI 2016! See the
&lt;a href="/pages/publications.html"&gt;publications&lt;/a&gt; page for the full paper. &lt;/p&gt;
&lt;p&gt;I think this work is interesting for three main reasons: the questions we addressed, the methodology for answering them, and the answers we arrived at. &lt;/p&gt;
&lt;p&gt;The fundamental question we addressed in this work is &lt;strong&gt;how do we engage differently with each other and with social media sites at different times and contexts&lt;/strong&gt;. In particular, 
we looked at several measures of engagement around times when people posted to Facebook. We drew on existing theories from communication and social
psychology to formulate hypotheses regarding the motivations for visiting Facebook, changes in the distribution of attention to content, and shifts in
decisions to interact with others.&lt;/p&gt;
&lt;p&gt;Our methodology for addressing the above questions is quite unique. We used a within-subject (i.e. comparing each person to herself), observational data analysis of deidentified log data of Facebook activity from a sample of 2.4 million people(!) over a period of 9 days. In our design, we observe the actions individuals chose to take on Facebook, without any intervention, around times of posting and another comparable activity, like liking
or commenting on another's post.&lt;/p&gt;
&lt;p&gt;The use of "Big Data" in this case adds to previous knowledge in a manner that is objective, representative of the phenomena as it happens in "real life", and that is very hard to measure otherwise. Previous work on the subject mostly relied on interviews and self-reported measures. Our use of a large sample of people in conjugation with objective measures of engagement allowed us to find small and medium effects that are unlikely to show up in interviews and surveys. In addition, it's hard to design an experiment that affects people's actions without inadvertently affecting the surrounding engagement around the action. &lt;/p&gt;
&lt;p&gt;Last but not least, our findings are &lt;strong&gt;really interesting&lt;/strong&gt;. We saw the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An increase in self-motivated site visits after posting.&lt;/li&gt;
&lt;li&gt;A small but substantive increase in the consumption and interaction with friends' content, but not others. &lt;/li&gt;
&lt;li&gt;A large and immediate effect of what we think is direct reciprocity.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To know more about these, you'd have to read the paper!&lt;/p&gt;
&lt;p&gt;The work is far from being perfect, of course, and as we discussed in the paper has major limitations. However, I believe that the work is a good first step in a direction to help Facebook and other social network providers think of ways to build better systems for people.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">grinbergnir</dc:creator><pubDate>Sun, 10 Jan 2016 12:39:00 -0500</pubDate><guid>tag:www.nirg.net,2016-01-10:contribution-effect.html</guid></item><item><title>WSDM 2014 Summary, Opinions and Other Thoughts</title><link>http://www.nirg.net/wsdm-2014-summary-opinions-and-other-thoughts.html</link><description>&lt;p&gt;It was a great attending WSDM this year, right at the heart of NYC! I could not possibly cover all that went down there in the three days of the conference, but would use this post to highlight sessions and talks that I attended and found particularly interesting.&lt;/p&gt;
&lt;p&gt;The conference itself was a single track mixture of mostly long, 20 minutes talks and few shorter talks. The sessions revolved around Web Search, Advertising, Recommender Systems, Network Analysis, Language Analysis and Crowdsourcing.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Web Search&lt;/strong&gt; Session was very good with several papers that took new and interesting perspectives to the “old" problem of search. In &lt;a href="#ref1"&gt;[1]&lt;/a&gt; Demeester et al. modeled disagreement between user rankings of top results and used it to improve ranking of search results. Another novel perspective was introduced by Hassan et al. in &lt;a href="#ref2"&gt;[2]&lt;/a&gt;, which learned to distinguish between long search sessions where the user is satisfied or struggling and unsatisfied. Li et al. looked in &lt;a href="#ref3"&gt;[3]&lt;/a&gt; at the phenomena of click spam: when spammer try to game search engines by automatically generating clicks on preferable results.&lt;/p&gt;
&lt;p&gt;Personally, I found the &lt;strong&gt;Advertising&lt;/strong&gt; sessions somewhat disappointing due to their (usual) focus on complex models that result marginal performance improvements, which rarely teach us something about users, ads or the search process. I get it -  someone has to pay the bills and minor improvements in modeling ads may lead to huge improvements in revenue. Nevertheless, I would much rather see models that actually teach us something about the process than just building a bigger, stronger hammer.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;Log Analysis&lt;/strong&gt; session was GREAT! I highly recommend looking at all papers, but just to pick a few: the best paper winner &lt;a href="#ref4"&gt;[4]&lt;/a&gt; by Lagun et al. identified common motifs in mouse movement over search results; &lt;a href="#ref5"&gt;[5]&lt;/a&gt; by Wang et al. learned jointly the user assignment to clusters and their resulting clicking behavior; &lt;a href="#ref6"&gt;[6]&lt;/a&gt; by Scaria et al. devised a game where participants have to get from a source wikipedia entry to a target entry by only following links and studied differences between successful and unsuccessful sessions.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;NLP&lt;/strong&gt;and &lt;strong&gt;Topic Modeling&lt;/strong&gt; sessions were pretty much standard LDA papers: we devised this graphical model, inferred it using Gibbs sampling and evaluated it by using perplexity on held-out. Two exception to that, can be found in &lt;a href="#ref7"&gt;[7]&lt;/a&gt; and &lt;a href="#ref8"&gt;[8]&lt;/a&gt;. Yu et al. used topics models in &lt;a href="#ref7"&gt;[7]&lt;/a&gt; to diversify E-commerce search results and evaluated its success in user satisfaction. Bi et al. found topic-specific experts in &lt;a href="#ref8"&gt;[8]&lt;/a&gt; and verified their results using external dataset.&lt;/p&gt;
&lt;p&gt;From the &lt;strong&gt;Peer Production; Data Analysis&lt;/strong&gt; session two papers stood out. First, the work by Abisheva et al. in &lt;a href="#ref9"&gt;[9]&lt;/a&gt; included a thorough analysis of the cross-section between YouTube and Twitter with interesting takes on the demographics of the platforms and identification of promotional account. Second, the paper by Di et al. in &lt;a href="#ref10"&gt;[10]&lt;/a&gt; looked at how image features such as the number images or their quality effect purchasing consumer decisions at eBay.&lt;/p&gt;
&lt;p&gt;Have any questions or comments? leave a comment below or contact me @grinbergnir.&lt;/p&gt;
&lt;h3&gt;References:&lt;/h3&gt;
&lt;p&gt;&lt;a name="ref1"&gt;&lt;/a&gt;[1] &lt;a href="http://wwwhome.ewi.utwente.nl/~hiemstra/papers/wsdm2014.pdf"&gt;Exploiting user disagreement for search evaluation: an experimental approach&lt;/a&gt;, Thomas Demeester, Robin Aly, Djoerd Hiemstra, Dong Nguyen, Dolf Trieschnigg, Chris Develder &lt;br/&gt;
&lt;a name="ref2"&gt;&lt;/a&gt;[2] &lt;a href="http://research.microsoft.com/en-us/um/people/ryenw/papers/HassanWSDM2014.pdf"&gt;Struggling or Exploring? Disambiguating Search Sessions&lt;/a&gt;, Ahmed Hassan, Ryen W. White, Susan T. Dumais, and Yi-Min Wang &lt;br/&gt;
&lt;a name="ref3"&gt;&lt;/a&gt;[3] &lt;a href="http://www.thuir.cn/group/~YQLiu/publications/wsdm2014.pdf"&gt;Search Engine Click Spam Detection Based on Bipartite Graph Propagation&lt;/a&gt;, Xin Li, Min Zhang, Yiqun Liu, Shaoping Ma, Yijiang Jin, Liyun Ru &lt;br/&gt;
&lt;a name="ref4"&gt;&lt;/a&gt;[4] &lt;a href="http://www.mathcs.emory.edu/~dlagun/pubs/motifs_wsdm2014.pdf"&gt;Discovering Common Motifs in Cursor Movement Data for Improving Web Search Ranking&lt;/a&gt;, Dmitry Lagun, Mikhail Ageev, Qi Guo, Eugene Agichtein &lt;br/&gt;
&lt;a name="ref5"&gt;&lt;/a&gt;[5] &lt;a href="http://sifaka.cs.uiuc.edu/~wang296/paper/wsdm488.pdf"&gt;User Modeling in Search Logs via A Nonparametric Bayesian Approach&lt;/a&gt;, Hongning Wang, ChengXiang Zhai, Feng Liang, Anlei Dong, Yi Chang &lt;br/&gt;
&lt;a name="ref6"&gt;&lt;/a&gt;[6] &lt;a href="http://cs.stanford.edu/people/jure/pubs/navigation-wsdm14.pdf"&gt;The Last Click: Why Users Give up Information Network Navigation&lt;/a&gt;, Aju Thalappillil Scaria, Rose Marie Philip, Robert West, Jure Leskovec &lt;br/&gt;
&lt;a name="ref7"&gt;&lt;/a&gt;[7] &lt;a href="http://web.engr.oregonstate.edu/~wong/papers/pdf/WSDM2014.pdf"&gt;Latent Dirichlet Allocation based Diversified Retrieval for E-commerce Search&lt;/a&gt;, Jun Yu, Sunil Mohan, Duangmanee Putthividhya, Weng-Keen Wong &lt;br/&gt;
&lt;a name="ref8"&gt;&lt;/a&gt;[8] &lt;a href="http://oak.cs.ucla.edu/~cho/papers/wsdm2014.pdf"&gt;Scalable Topic-Specific Influence Analysis on Microblogs&lt;/a&gt;, Bin Bi, Yuanyuan Tian, Yannis Sismanis, Andrey Balmin, Junghoo Cho &lt;br/&gt;
&lt;a name="ref9"&gt;&lt;/a&gt;[9] &lt;a href="http://ingmarweber.de/wp-content/uploads/2014/02/Who-Watches-and-Shares-What-on-YouTube-And-When-Using-Twitter-to-Understand-YouTube-Viewership.pdf"&gt;Who Watches (and Shares) What on YouTube? And When? – Using Twitter to Understand YouTube Viewership&lt;/a&gt;, Adiya Abisheva, Venkata R. Kiran Garimella, David Garcia, Ingmar Weber &lt;br/&gt;
&lt;a name="ref10"&gt;&lt;/a&gt;[10] &lt;a href="http://labs.ebay.com/events/event/news-vision-paper-accepted-at-wsdm-2014"&gt;Is a picture really worth a thousand words? – on the role of images in e-commerce&lt;/a&gt;, Wei Di, Neel Sundaresan, Robinson Piramuthu, Anurag Bhardwaj &lt;br/&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">grinbergnir</dc:creator><pubDate>Fri, 28 Feb 2014 08:21:00 -0500</pubDate><guid>tag:www.nirg.net,2014-02-28:wsdm-2014-summary-opinions-and-other-thoughts.html</guid></item><item><title>Our Semantic Keyword Expansion API is ON!</title><link>http://www.nirg.net/sem-exp-api.html</link><description>&lt;p&gt;If you ever wanted to track a certain topic on Twitter, you probably wondered what keywords people are using to refer to it. Our new Semantic Expansion API lets you do exactly that - given a short list of keywords, it finds keywords that appear in a similar context or directly with the initial list. How is this useful? by tracking more keywords you get a more robust and complete coverage of desired topics. What is "semantic" about it? using co-occurrence patterns from individual tweets lets us preserve the semantic structure already embedded in tweets and capitalize on it for keyword expansion. In this blog post I'll briefly describe our expansion methods, the methods' API and provide examples for using it from Python.&lt;/p&gt;
&lt;h2&gt;Expansion Methods&lt;/h2&gt;
&lt;p&gt;Two of the coolest outcomes of our work "&lt;em&gt;Extracting Diurnal Patterns of Real World Activity from Social Media&lt;/em&gt;" are methods for semantically expanding a list of keywords using Twitter co-occurrence patterns. Raw co-occurrence counts of words can be noisy sometimes and so our methods use the more stable &lt;a href="http://en.wikipedia.org/wiki/Pointwise_mutual_information"&gt;Pointwise Mutual Information (PMI)&lt;/a&gt;. Our methods traverse the PMI graph (image above) in several ways and select keywords for inclusion.&lt;/p&gt;
&lt;p&gt;The Aggregated PMI (APMI) method consider terms appearing directly withthe initial set of keywords. For example, when starting from the hashtag"#aurora" (as in the PMI graph above), APMI may consider including"shooting", "colorado" and "theater" in the expanded list. Our secondexpansion method, ContextPMI, examines the context of initial seed listand find keywords that have similar context. Following on the previousexample, ContextPMI would look for keywords with "shooting", "colorado"and "theater" in their context.&lt;/p&gt;
&lt;p&gt;The actual methods are more involved than described here: we need tohandle common and corpus-specific stopwords and exclude words relevantonly to a small subset of the initial seed list. The interested readershould refer to the &lt;a href="/papers/grinberg-icwsm2013-extracting.pdf" target="_blank"&gt;full paper&lt;img src="/images/pdf-icon-16x16.png"&gt;&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h2&gt;Semantic Expansion API&lt;/h2&gt;
&lt;p&gt;Our Semantic Expansion API provide two endpoints: AGG_PMI and CONTEXT_PMI, which are shortens for the Aggregated PMI and Context PMImethods described earlier. Both methods are accessible through HTTP GET requests, taking a list of keywords for expansion as input and return an expanded list of terms with their scores. The two endpoints support few other parameters:&lt;/p&gt;
&lt;h5&gt;AGG_PMI Endpoint&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;terms (required): comma-separated list of keywords for expansion as string (url encoded).&lt;/li&gt;
&lt;li&gt;top_n: number of top terms to return, according to their score. Defaults to 100.&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;CONTEXT_PMI Endpoint&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;terms (required): comma-separated list of keywords for expansion as string (url encoded).&lt;/li&gt;
&lt;li&gt;freq_cutoff: minimum document frequency of returned terms over a period of two months. Defaults to 200 and values below that will be ignored.&lt;/li&gt;
&lt;li&gt;bootstrap_samples: integer, number of times to bootstrap the seed list in building the context for the method. Defaults to 100.&lt;/li&gt;
&lt;li&gt;score_cutoff: integer, specifying the minimum score for returned terms. Defaults to 500.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Both methods return a list of terms and their corresponding scores as a json object.&lt;/p&gt;
&lt;h2&gt;Using the API from Python&lt;/h2&gt;
&lt;p&gt;As a Python enthusiast, it was natural for me to write a service client in python. The &lt;em&gt;semexpweb.py&lt;/em&gt; module has &lt;em&gt;Client&lt;/em&gt; class, which provide a convenient, pythonic way for querying the API. To instantiate a Client object do the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;semexpweb&lt;/span&gt;  
&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;semexpweb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Client&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;Contact me for server info&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can also specify a timeout for the HTTP request (defaults to 60). The IP above points to our server and should rarely change.&lt;/p&gt;
&lt;p&gt;Once you have the client object you can give it a list of keywords for expansion, in this example a list of coffee related terms:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;coffee_terms&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;coffee&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#coffee&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;starbucks&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#starbucks&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \\  
                &lt;span class="s"&gt;&amp;#39;espresso&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;\#espresso&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;lovecoffee&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#lovecoffee&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \\  
                &lt;span class="s"&gt;&amp;#39;caffeineaddict&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;\#caffeineaddict&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;venti&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#venti&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \\  
                &lt;span class="s"&gt;&amp;#39;starbucks&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#starbucks&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;mugs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#mugs&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;latte&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; \\  
                &lt;span class="s"&gt;&amp;#39;#latte&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;caf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#caf&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;coffeebean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;#coffeebean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;context_pmi&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coffee_terms&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output should look like this:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;#mocha&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2656&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;#barista&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2570&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;#cappuccino&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2565&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;#starbuck&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2231&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;#caramel&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2183&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;#frappuccino&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2103&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;#iced&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1892&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;#latteart&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1860&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;macchiatos&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1776&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;macchiato&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1769&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;u&amp;#39;frappucino&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1738&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Source Code&lt;/h2&gt;
&lt;p&gt;The source code for using the API as described in this post is in &lt;a href="/files/SemExpWeb.zip" target="_blank"&gt;SemExpWeb.zip&lt;img src="/images/zip-icon-16x16.png"&gt;&lt;/a&gt;. You would find in the archive both the &lt;em&gt;semexpweb.py&lt;/em&gt; module and &lt;em&gt;usage_example.py&lt;/em&gt; containing the example code in this post.&lt;/p&gt;
&lt;p&gt;Have any questions or comments? leave a comment down below or contact me @grinbergnir.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">grinbergnir</dc:creator><pubDate>Sun, 19 May 2013 16:36:00 -0500</pubDate><guid>tag:www.nirg.net,2013-05-19:sem-exp-api.html</guid></item><item><title>Best Practices for Querying Twitter API using (forked) tweepy</title><link>http://www.nirg.net/using-tweepy.html</link><description>&lt;p&gt;Over the past few weeks I have been accessing Twitter API from python
using tweepy. In the course of doing so, I added support in tweepy for
using multiple authentication handlers, monitoring rate limits, waiting
when running out of calls, support for search using Twitter API v1.1,
proper pagination of results and more. In this blog post I will describe
the major changes in my forked version of the tweepy package and provide
some best practices and examples for querying the Twitter API robustly.
Fork me on &lt;a href="http://github.com/nirg/tweepy"&gt;GitHub&lt;/a&gt; or follow my pull request on the main tweepy repo
&lt;a href="http://github.com/tweepy/tweepy/pull/282"&gt;#282&lt;/a&gt; to get the complete code referenced in this post.&lt;/p&gt;
&lt;h2&gt;New Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Support for multiple authentication handlers&lt;/strong&gt;: sometimes you just
    need more than a single authentication handler. You can provide a
    list of authentication handlers to tweepy and manually switch
    between them:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;tweepy&lt;/span&gt;

&lt;span class="c"&gt;# create authentication handlers given pre-existing keys  &lt;/span&gt;
&lt;span class="n"&gt;auths&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;  
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;consumer_key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consumer_secret&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;access_key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;access_secret&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;oauth_keys&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
    &lt;span class="n"&gt;auth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OAuthHandler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;consumer_key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consumer_secret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;auth&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_access_token&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;access_key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;access_secret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
    &lt;span class="n"&gt;auths&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auth&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;api&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;API&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auths&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c"&gt;# switch to the second authentication handler (zero-based index)  &lt;/span&gt;
&lt;span class="n"&gt;api&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;auth_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Monitoring rate limit&lt;/strong&gt;: switching authentication handlers
    manually is nice, but it is far more useful to have tweepy do that
    automatically based on the number of remaining api calls.
    The &lt;code&gt;monitor_rate_limit&lt;/code&gt; argument does exactly that - monitor to
    number of remaining calls to the Twitter API endpoint and switch to
    the next authentication handler when running out of calls. This
    feature lets you be a good citizen and not supersede your API calls
    quota.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;api&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;API&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auths&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;monitor_rate_limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Blocking when running out of calls&lt;/strong&gt;: using the argument
    wait_on_rate_limit you can have tweepy wait when running out of
    calls. By monitoring the http headers returned from Twitter, tweepy
    now knows exactly how long to wait until the next lump of api calls
    is available. This is useful when you need more than the number of
    calls allowed by the rate limit, but don't want your program to fail
    with an exception or lose track of the current position in your
    results.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;api&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;API&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auths&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;monitor_rate_limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wait_on_rate_limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Proper iteration of result pages (pagination)&lt;/strong&gt;: prior to my
    forked version of tweepy, if you used the Cursor object to iterate
    over results you probably have missed some tweets in your result
    set. A good explanation of why this has happened is in Twitter own
    documentation on &lt;a href="https://dev.twitter.com/docs/working-with-timelines"&gt;working with timelines&lt;/a&gt;. Now iteration uses
     &lt;code&gt;max_id&lt;/code&gt; parameter to properly traverse results when using the
    cursor object. If you're wondering what's happening behind the
    scenes, take a look at &lt;code&gt;MaxIdIterator&lt;/code&gt; class in &lt;code&gt;tweepy/cursor.py&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Search endpoint now supports Twitter API v1.1&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unit testing for all of the above&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Best Practices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Retrying on errors&lt;/strong&gt;: little know fact, unless you don't dig into
    the tweepy code, is that you can have tweepy retry making calls for
    you when certain error codes are returned. The retry feature is
    useful for handling temporary failures. You specify how many retries
    will be performed, delay in seconds between calls and the error
    response codes upon which tweepy will retry making calls. For a
    complete list of error response codes, see the Twitter API
    documentation on &lt;a href="https://dev.twitter.com/docs/error-codes-responses" title="Error Codes &amp;amp; Responses"&gt;Error Codes &amp;amp; Responses&lt;/a&gt;. Here is an example of
    using the retry feature:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;api&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;API&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;auths&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;retry_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;retry_delay&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                 &lt;span class="n"&gt;retry_errors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;401&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;404&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;503&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;How to use pagination with search?&lt;/strong&gt; here is the example from
    &lt;code&gt;examples/paginated_search.py&lt;/code&gt;:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;auth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OAuthHandler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;consumer_key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consumer_secret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;auth&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_access_token&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;access_token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;access_token_secret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;api&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;API&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;auth&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; 
        &lt;span class="c"&gt;# support for multiple authentication handlers  &lt;/span&gt;
        &lt;span class="c"&gt;# retry 3 times with 5 seconds delay when getting these error codes&lt;/span&gt;
        &lt;span class="c"&gt;# For more details see &lt;/span&gt;
        &lt;span class="c"&gt;# https://dev.twitter.com/docs/error-codes-responses  &lt;/span&gt;
        &lt;span class="n"&gt;retry_count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;retry_delay&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;retry_errors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;401&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;404&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;503&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; 
        &lt;span class="c"&gt;# monitor remaining calls and block until replenished  &lt;/span&gt;
        &lt;span class="n"&gt;monitor_rate_limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;wait_on_rate_limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt; 
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;cupcake OR donut&amp;#39;&lt;/span&gt;  
&lt;span class="n"&gt;page_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;  
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tweets&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Cursor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;api&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="n"&gt;result_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;recent&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;include_entities&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pages&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;  
&lt;span class="n"&gt;page_count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  
&lt;span class="c"&gt;# print just the first tweet out of every page of 100 tweets  &lt;/span&gt;
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;tweets&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="c"&gt;# stop after retrieving 200 pages  &lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;page_count&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  
    &lt;span class="k"&gt;break&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Response as python dictionary&lt;/strong&gt;: sometimes you don't want tweepy
    to spend time on parsing json into dedicated python object and have
    the results simply as a dictionary. The example at
    &lt;code&gt;examples/json_results.py&lt;/code&gt; demonstrate querying a user timeline, tim
    oreilly in this case, using a json payload type:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;auth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OAuthHandler&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;consumer_key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;consumer_secret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
&lt;span class="n"&gt;auth&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_access_token&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;access_token&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;access_token_secret&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;api&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;API&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;auth&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  
&lt;span class="n"&gt;json_user_timeline&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;bind_api&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;  
    &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;/statuses/user_timeline.json&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  
    &lt;span class="n"&gt;payload_type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;json&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;payload_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  
    &lt;span class="n"&gt;allowed_param&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;user_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;screen_name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;since_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="s"&gt;&amp;#39;max_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;page&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;include_rts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c"&gt;# iterate over 50 of tim oreilly&amp;#39;s tweets  &lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tweet&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tweepy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Cursor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json_user_timeline&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;api&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;screen_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;timoreilly&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;include_rts&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;  
&lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="n"&gt;tweet&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Have a comment? an idea of how to do things more elegantly? leave a comment or contact me on twitter @grinbergnir.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">grinbergnir</dc:creator><pubDate>Sun, 28 Apr 2013 17:33:00 -0500</pubDate><guid>tag:www.nirg.net,2013-04-28:using-tweepy.html</guid></item><item><title>Extracting Diurnal Patterns of Real World Activity from Social Media</title><link>http://www.nirg.net/extracting-diurnal-patterns.html</link><description>&lt;p&gt;Our most recent work got accepted to ICWSM 2013! See the
&lt;a href="/pages/publications.html"&gt;publications&lt;/a&gt; page for the full paper.&lt;/p&gt;
&lt;p&gt;Here is the abstract:&lt;br /&gt;
In this study, we develop methods to identify verbal expressions in
social media streams that refer to real-world activities. Using
aggregate daily patterns of Foursquare checkins, our methods extract
similar patterns from Twitter, extending the amount of available content
while preserving high relevance. We devise and test several methods to
extract such content, using time-series and semantic similarity.
Evaluating on key activity categories available from Foursquare (coffee,
food, shopping and nightlife), we show that our extraction methods are
able to capture equivalent patterns in Twitter. By examining rudimentary
categories of activity such as nightlife, food or shopping we peek at
the fundamental rhythm of human behavior and observe when it is
disrupted. We use data compiled during the abnormal conditions in New
York City throughout Hurricane Sandy to examine the outcome of our
methods.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">grinbergnir</dc:creator><pubDate>Sat, 30 Mar 2013 12:39:00 -0500</pubDate><guid>tag:www.nirg.net,2013-03-30:extracting-diurnal-patterns.html</guid></item></channel></rss>